{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mask operations**\n",
    "\n",
    "Here we explore different operations with image masks and tricks that can be helpful in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "The usual preparations... Before we begin, let's load some drawing functions for rendering images effortlessly in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "# Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Functionality related to this course\n",
    "sys.path.append(\"..\")\n",
    "import tools\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image data. We use the below image of red and white blood cells. We previously have created a segmentation mask for the red blood cells in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default dataset (feel free to change)\n",
    "mask = cv.imread(\"../data/images/hematology-baso1-mask.png\", cv.IMREAD_GRAYSCALE)\n",
    "img = cv.imread(\"../data/images/hematology-baso1.jpg\", cv.IMREAD_COLOR)\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "tools.show_image_chain([img, mask], titles=[\"Image\", \"Mask (red blood cells)\"], suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the effect of some operations, we will use a noisy version of that mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy version of the mask\n",
    "np.random.seed(1)\n",
    "mask_noisy = mask.copy()\n",
    "mask_noisy[np.random.rand(*mask_noisy.shape) < 0.1] = 0\n",
    "tools.show_image(mask_noisy, suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hole filling**\n",
    "\n",
    "Hole filling refers to the process of filling in holes in the mask. For small holes, we can use morphological operations. For larger holes, we can use the outer contours of the components and fill them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Morphological operations\n",
    "kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (3, 3))\n",
    "mask_filled1 = cv.morphologyEx(mask_noisy, cv.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "\n",
    "# Method 2: Contour filling\n",
    "mask_filled2 = mask_noisy.copy()\n",
    "contour, hierarchy = cv.findContours(mask_noisy, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE)\n",
    "for cnt in contour:\n",
    "    cv.drawContours(mask_filled2,[cnt],0,255,-1)\n",
    "\n",
    "\n",
    "# Visalize the results\n",
    "tools.show_image_chain([mask_noisy, mask_filled1, mask_filled2], \n",
    "                       titles=[\"Noisy mask (input)\", \"Filled mask (morphology)\", \"Filled mask (contour)\"],\n",
    "                       suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Flood fill**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we want to fill the mask from a certain pixel with a certain gray level (e.g., 255), but only if the pixel is connected to the top-left corner. This can be achieved using the flood fill algorithm.\n",
    "\n",
    "In the below example, we start the flood fill from the pixel indicated by the red point (seed point) at pixel (30, 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the seed point\n",
    "seed_point = (30, 30)\n",
    "\n",
    "# Flood fill\n",
    "mask_filled = mask_noisy.copy()\n",
    "cv.floodFill(mask_filled, None, seed_point, 255)\n",
    "\n",
    "# Visualize the seed point as a red dot\n",
    "mask_filled_with_seed = cv.cvtColor(mask_filled, cv.COLOR_GRAY2RGB)\n",
    "cv.circle(mask_filled_with_seed, seed_point, 7, [255,0,0], -1)\n",
    "tools.show_image_chain([mask_noisy, mask_filled_with_seed, None], \n",
    "                       titles=[\"Input\", \"Filled mask (flood fill)\", None],\n",
    "                       suppress_info=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can use an additional mask to restrict the flood fill. Note that the restriction mask should be two pixels larger in each dimension than the image, a requirement of `cv.floodFill()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply flood fill with a restriction mask\n",
    "mask_filled = mask_noisy.copy()\n",
    "mask_restriction = np.zeros((mask_filled.shape[0]+2, mask_filled.shape[1]+2), dtype=np.uint8)\n",
    "mask_restriction = cv.circle(mask_restriction, (150, 150), 125, 255, 3)\n",
    "mask_restriction = cv.rectangle(mask_restriction, (250, 100), (450, 300), 255, 3)\n",
    "mask_restriction = cv.ellipse(mask_restriction, (500, 330), (100, 80), 30, 0, 360, 255, 3)\n",
    "cv.floodFill(mask_filled, mask_restriction, (0, 0), 255)\n",
    "\n",
    "# Visualize the seed point as a red dot\n",
    "mask_filled_with_seed = cv.cvtColor(mask_filled, cv.COLOR_GRAY2RGB)\n",
    "cv.circle(mask_filled_with_seed, seed_point, 7, [255,0,0], -1)\n",
    "tools.show_image_chain([mask_noisy, mask_restriction, mask_filled_with_seed], \n",
    "                       titles=[\"Input\", \"Mask\", \"Filled mask (flood fill)\"],\n",
    "                       suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous example, the flood fill is obstructed not only by the mask, but also by the image content. This is why the flood fill does not fill the entire image. Only the pixels that are connected (= neighboring pixels with the same pixel value) to the seed point are filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Connected components**\n",
    "\n",
    "Assume you have a binary segmentation mask with multiple objects in it. \n",
    "The goal is to find the number of objects and their bounding boxes.\n",
    "\n",
    "We can use OpenCV's `cv.connectedComponents()` to label the connected components. The function returns an image with the same size as the input image, where each pixel has a label corresponding to the connected component it belongs to. Background pixels are labeled with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: OpenCV\n",
    "# Connectivity: 4 or 8. This parameter specifies how pixels are connected.\n",
    "# In the 4-connected case, two pixels are connected if they share an edge.\n",
    "# In the 8-connected case, two pixels are connected if they share an edge or a corner.\n",
    "labels = cv.connectedComponents(mask, connectivity=8)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's about it. The rest of this section is about how the various components can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for visualization\n",
    "results = {}\n",
    "results[\"Input\"] = mask\n",
    "\n",
    "# Visualization 1: \n",
    "# ################\n",
    "# Display the connected components in different colors.\n",
    "def colorize_labels(labels):\n",
    "    \"\"\"\n",
    "    Represent the data in HSV such that the connected components are colored\n",
    "    differently. Recall: HSV refers to hue, saturation, and value. Hue \n",
    "    represents an angle in the color wheel, saturation the intensity of the \n",
    "    color, and value the brightness. To represent black, we set value to zero.\n",
    "    \"\"\"\n",
    "    h = labels/labels.max()*128\n",
    "    s = np.ones_like(labels)*255\n",
    "    v = (labels > 0) * 255\n",
    "    labels_color = np.stack([h, s, v], axis=-1)\n",
    "    labels_color = cv.cvtColor(labels_color.astype(np.uint8),\n",
    "                               cv.COLOR_HSV2RGB)\n",
    "    return labels_color\n",
    "\n",
    "labels_color = colorize_labels(labels)\n",
    "results[\"Connected components\"] = labels_color\n",
    "\n",
    "# Visualization 2: \n",
    "# ################\n",
    "# Restrict the number of colors and shuffle them. \n",
    "# We can use a look-up table (LUT) for this purpose.\n",
    "def colorize_labels_random(labels):\n",
    "    h_vals = [1, 34, 55, 99]  \n",
    "    assert 0 not in h_vals  #Â Zero is reserved for the background\n",
    "    # For reproducability\n",
    "    np.random.seed(1)\n",
    "    # Sample n times with replacement from h_vals \n",
    "    lut = np.random.choice(h_vals, labels.max())\n",
    "    # Insert a zero at the beginning to represent the background\n",
    "    lut = np.insert(lut, 0, 0)\n",
    "    # Lookup values\n",
    "    labels_new = lut[labels]\n",
    "    labels_color = colorize_labels(labels_new)\n",
    "    return labels_color\n",
    "\n",
    "labels_color = colorize_labels_random(labels)\n",
    "results[\"Randomized colors\"] = labels_color\n",
    "\n",
    "tools.show_image_grid(results, figsize=(10, 10), suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is powered by `cv.connectedComponents()`. Note that we can also use  `scipy.ndimage.label()` for this purpose. The latter is more flexible and can be used for n-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.ndimage as si\n",
    "labels, n_labels = si.label(mask)\n",
    "\n",
    "# It's output is equivalent to cv.connectedComponents().\n",
    "ret = colorize_labels(labels)\n",
    "\n",
    "# Compare the results\n",
    "tools.show_image_chain([results[\"Connected components\"], ret],\n",
    "                       titles=[\"OpenCV: cv.connectedComponents()\", \n",
    "                               \"Scipy: scipy.ndimage.label()\"], \n",
    "                       suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to calculate different properties for each of the extracted components. Before we do that, let's first introduce some other concepts such as the contour or convex hull of a shape that will be useful for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Contours**\n",
    "\n",
    "Contours are the boundaries of connected components. We can extract contours from a binary image directly using `cv.findContours()`.\n",
    "\n",
    "The function returns two values: a list of contours and hierarchy information. The hierarchy is a list of four indices that describe the relationship between the contours: [next, previous, first child, parent]. Each of these four values is an index that identifies the corresponding contour. If there is no such element, the value is -1. \n",
    "\n",
    "In the following, we will not use the hierarchy information, so we will ignore the second return value. Hierarchy information is useful when contours are nested, i.e. when there are contours within other contours. \n",
    "\n",
    "There are different modes to retrieve the contours (argument `mode`). The most common ones are:\n",
    "- `cv.RETR_EXTERNAL`: Only the external contours are returned.\n",
    "- `cv.RETR_LIST`: All contours are returned in a flat list \n",
    "- `cv.RETR_CCOMP`: All contours are returned in a two-level hierarchy (external and holes)\n",
    "- `cv.RETR_TREE`: All contours are returned in a tree hierarchy\n",
    "\n",
    "Finally, the parameter `method` specifies how the contours are approximated (or simplified). The main goal here is to reduce the number of points while maintaining the shape of the contour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv.findContours(mask, \n",
    "                              mode=cv.RETR_EXTERNAL, \n",
    "                              method=cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `cv.drawContours()` we can draw (and fill) contours conveniently. In the following, we demonstrate different ways to visualize contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Input\"] = mask\n",
    "\n",
    "# Visualization 1: Simple contours\n",
    "# ################################\n",
    "img_outlines = np.zeros_like(img)\n",
    "color = [255, 255, 0]\n",
    "cv.drawContours(img_outlines, contours, \n",
    "                contourIdx=-1, \n",
    "                color=color, \n",
    "                thickness=2)\n",
    "results[\"Plain contours\"] = img_outlines\n",
    "\n",
    "# Visualization 2: Single contour (filled)\n",
    "# ########################################\n",
    "img_outlines = np.zeros_like(img)\n",
    "color = [50, 100, 255]\n",
    "contour_id = 42\n",
    "cv.drawContours(img_outlines, contours, \n",
    "                contourIdx=contour_id, \n",
    "                color=color, \n",
    "                thickness=-1)\n",
    "results[\"Single contour (filled)\"] = img_outlines\n",
    "\n",
    "# Visualization 3: Use colors\n",
    "# ###########################\n",
    "import matplotlib as mpl\n",
    "# Use colormaps from matplotlib\n",
    "# https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "cmap = mpl.colormaps[\"inferno\"]\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(contours))]\n",
    "# Convert colors from [0, 1] to [0, 255]\n",
    "colors = [[int(c*255) for c in color] for color in colors]\n",
    "img_outlines = img.copy()\n",
    "for i, contour in enumerate(contours):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i], \n",
    "                    thickness=3)\n",
    "results[\"Contours in color\"] = img_outlines\n",
    "\n",
    "# Visualization 4: Random colors\n",
    "# ##############################\n",
    "img_outlines = img.copy()\n",
    "contours_shuffled = list(contours)\n",
    "#np.random.shuffle(contours_shuffled)\n",
    "colors = [[50, 100, 255],\n",
    "          [255, 100, 50],\n",
    "          [50, 255, 100]]\n",
    "for i, contour in enumerate(contours_shuffled):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours_shuffled, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i % len(colors)], \n",
    "                    thickness=3)\n",
    "results[\"Randomized colors\"] = img_outlines\n",
    "\n",
    "# Visualization 5: Colorize based on size\n",
    "# #######################################\n",
    "sizes = np.array([cv.contourArea(contour) for contour in contours])\n",
    "sizes = sizes/sizes.max()  # Normalized sizes\n",
    "img_outlines = img.copy()\n",
    "# Use the following line if you want to sort the contours by size\n",
    "# contours_sorted = sorted(contours, key=cv.contourArea)\n",
    "\n",
    "# Use the colormap \"hsv\". It contains a transition red-yellow-green.\n",
    "# cmap is a function that maps a scalar between 0 and 1 to a color.\n",
    "# To sample only colors in the red-yellow-green range, we can use only\n",
    "# the first 30% of the colormap. \n",
    "cmap = mpl.colormaps[\"hsv\"]\n",
    "colors = [cmap(s*0.3) for s in sizes]\n",
    "colors = [[int(c*255) for c in color] for color in colors]\n",
    "\n",
    "for i, contour in enumerate(contours):\n",
    "    cv.drawContours(img_outlines, \n",
    "                    contours, \n",
    "                    contourIdx=i, \n",
    "                    color=colors[i % len(colors)], \n",
    "                    thickness=-1)  # Filled\n",
    "results[\"Colors sorted by size\"] = img_outlines\n",
    "\n",
    "# Visualization 6: Bounding boxes and centroids\n",
    "# #############################################\n",
    "def draw_bounding_boxes(img, contours):\n",
    "    assert img.ndim == 3\n",
    "    img = img.copy()\n",
    "    cmap = mpl.colormaps[\"hsv\"]\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, len(contours))]\n",
    "    colors = [[int(c*255) for c in color] for color in colors]\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv.boundingRect(contour)\n",
    "        cv.rectangle(img, (x, y), (x+w, y+h), \n",
    "                    color=colors[i], thickness=2)\n",
    "        \n",
    "        # For the center of mass, we need to compute the moments of the contour\n",
    "        # https://docs.opencv.org/4.x/dd/d49/tutorial_py_contour_features.html\n",
    "        M = cv.moments(contour)\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv.circle(img, (cx, cy), 5, color=colors[i], thickness=-1)\n",
    "    return img\n",
    "\n",
    "img_in = cv.cvtColor(mask, cv.COLOR_GRAY2RGB)\n",
    "ret = draw_bounding_boxes(img_in, contours)\n",
    "results[\"Bounding boxes\"] = ret\n",
    "\n",
    "ret = draw_bounding_boxes(img, contours)\n",
    "results[\"Bounding boxes (as overlay)\"] = ret\n",
    "\n",
    "tools.show_image_grid(results, figsize=(10, 9), suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convex hull**\n",
    "\n",
    "A convex object is a set of points where the line segment connecting *any two points*\n",
    "in the set lies always entirely within the set. See below for examples of convex and\n",
    "non-convex objects.\n",
    "\n",
    "![Convex and non-convex shapes](../data/doc/convexity.svg)\n",
    "\n",
    "**Figure**: Illustration of non-convex (left) and convex shapes (middle and right). Image source: [Link](https://d2l.ai/index.html)\n",
    "\n",
    "The **convex hull** of a geometric object (or a set of points) is the smallest\n",
    "convex shape that contains the object. The convex hull of a set of points\n",
    "can be computed using the OpenCV function `cv.convexHull()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the convex hull for a binary mask\n",
    "mask_text = cv.imread(\"../data/images/word-ice-cream.png\", cv.IMREAD_GRAYSCALE)\n",
    "mask_text = 255 - mask_text  # Invert the mask\n",
    "contours_text, _ = cv.findContours(mask_text, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "drawing = cv.cvtColor(mask_text, cv.COLOR_GRAY2RGB)\n",
    "color = (255, 0, 0)\n",
    "for i in range(len(contours_text)):\n",
    "    hull = cv.convexHull(contours_text[i])\n",
    "    cv.drawContours(drawing, [hull], 0, color=color, thickness=1)\n",
    "\n",
    "tools.show_image_chain([mask_text, drawing], ncols=1,\n",
    "                       titles=[\"Input\", \"Convex hull\"], \n",
    "                       suppress_info=True, \n",
    "                       figsize=(6, 5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our initial mask with the red blood cells, the convex hulls look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the convex hull for a binary mask\n",
    "drawing = img.copy()\n",
    "color = (255, 255, 0)\n",
    "for i in range(len(contours)):\n",
    "    hull = cv.convexHull(contours[i])\n",
    "    cv.drawContours(drawing, [hull], 0, color=color, thickness=2)\n",
    "\n",
    "tools.show_image_chain([mask, drawing], \n",
    "                       titles=[\"Input\", \"Convex hull\"], \n",
    "                       suppress_info=True, \n",
    "                       figsize=(8, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Distance transform**\n",
    "\n",
    "The distance transform computes the distance of each pixel to the nearest zero pixel. The distance transform can be useful for shape analysis or preprocessing.\n",
    "\n",
    "Below we demonstrate how the distance transformations are calculated for two different mask images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Input 1\"] = mask\n",
    "results[\"Input 2\"] = mask_text\n",
    "\n",
    "# Compute the distance transform\n",
    "dist_transform = cv.distanceTransform(mask, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "dist_transform /= dist_transform.max()\n",
    "results[\"Distance transform 1\"] = dist_transform\n",
    "\n",
    "dist_transform = cv.distanceTransform(mask_text, cv.DIST_L2, cv.DIST_MASK_PRECISE)\n",
    "dist_transform /= dist_transform.max()\n",
    "results[\"Distance transform 2\"] = dist_transform\n",
    "\n",
    "tools.show_image_grid(results, ncols=2, figsize=(10, 5), suppress_info=True, shape=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance transform results in a grayscale image that represents the distance to the nearest zero pixel. The brighter the pixel, the further away it is from the nearest zero pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Thinning**\n",
    "Thinning (or skeletonization) is the process of reducing a binary image to a skeleton representation. The skeleton is a thin representation of the object that is useful for shape analysis. It can be computed using the Zhang-Suen algorithm ([link](https://doi.org/10.1145/357994.358023)), which is implemented in the opencv-contrib-python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the skeleton (requires the opencv-contrib-python package)\n",
    "if (not hasattr(cv, \"ximgproc\")) or (not hasattr(cv.ximgproc, \"thinning\")):\n",
    "    raise RuntimeError(\"This version of OpenCV does not support thinning.\")\n",
    "\n",
    "\n",
    "thinned = cv.ximgproc.thinning(mask_text)\n",
    "ret = cv.cvtColor(mask_text, cv.COLOR_GRAY2RGB)\n",
    "ret[thinned == 255] = [0, 0, 255]\n",
    "\n",
    "tools.show_image_chain([mask_text, ret], ncols=1, \n",
    "                       titles=[\"Input (inverted mask)\", \n",
    "                               \"Skeletonization of the mask\"], \n",
    "                       suppress_info=True, figsize=(6, 5));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use thinning not only to reduce a binary image to a skeleton, but also\n",
    "find paths that are equidistant to the boundaries of the objects in the image. \n",
    "You can see an example below. Note that we calculate the thinning for the background mask: `mask_bg = (255-mask)`. \n",
    "\n",
    "The lines in the below example indicate pixels that are at the same distance from (at least) two contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute thinning on the background mask\n",
    "thinned = cv.ximgproc.thinning(255-mask)\n",
    "ret = img.copy()\n",
    "ret[thinned == 255] = [0, 0, 0]\n",
    "\n",
    "# Visualize the results\n",
    "tools.show_image_chain([mask, ret], ncols=2, \n",
    "                       titles=[\"Input (inverted mask)\", \n",
    "                               \"Skeletonization of the inverted mask\"], \n",
    "                       suppress_info=True, figsize=(7, 12));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Measuring components**\n",
    "\n",
    "We sometimes need to compute geometric properties per component or contour. For instance, we may want to compute the center of mass, area, perimeter, circularity, etc. for each of the red blood cells in the mask in our sample dataset. The following code demonstrates how to compute these properties.\n",
    "\n",
    "Circularity $c$ measures how close the shape of an object is to a circle. It is defined as \n",
    "$$c = \\frac{4\\cdot \\pi\\cdot A}{p^2}$$\n",
    "Note that that the circle is the shape with the smallest perimeter for a given area, which is why the circularity $c\\leq 1$, where $c=1$ for a perfect circle. We can use this metric to distinguish rather round objects from elongated or irregular shapes. \n",
    "\n",
    "Of course, we can calculate many other metrics that describe the shape of an object (so-called shape descriptors). Such parameters are usually very domain-specific.\n",
    "\n",
    "Note that component 0 in the below result table refers to the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the area of each connected component\n",
    "areas = np.bincount(labels.flatten())\n",
    "n_labels = labels.max()\n",
    "\n",
    "# Compute the perimeter of each connected component\n",
    "perimeters = np.zeros(n_labels+1)\n",
    "areas_1 = np.zeros(n_labels+1)\n",
    "areas_2 = np.zeros(n_labels+1)\n",
    "circularities = np.zeros(n_labels+1)\n",
    "centers = np.zeros((n_labels+1, 2))\n",
    "\n",
    "for i in range(1, n_labels+1):\n",
    "    mask_i = (labels == i).astype(np.uint8)\n",
    "    contours, _ = cv.findContours(mask_i, \n",
    "                                  cv.RETR_EXTERNAL, \n",
    "                                  cv.CHAIN_APPROX_SIMPLE)\n",
    "    assert len(contours) == 1  # We expect only one contour\n",
    "    # Perimeter\n",
    "    perimeters[i] = cv.arcLength(contours[0], closed=True)\n",
    "    \n",
    "    # Area of the contour (measured using the Green's theorem)\n",
    "    areas_1[i] = cv.contourArea(contours[0])\n",
    "    # Area of the contour (measured as the number of pixels)\n",
    "    areas_2[i] = np.sum(mask_i)\n",
    "    # Note: The two area measures are not always the same!\n",
    "    \n",
    "    # Circularity \n",
    "    circularities[i] = 4*np.pi*areas[i]/(perimeters[i]**2)\n",
    "    \n",
    "    # Center of mass\n",
    "    centers[i, :] = np.mean(np.argwhere(mask_i), axis=0)\n",
    "\n",
    "# Collect the info in a pandas DataFrame\n",
    "df = pd.DataFrame({\"Area\": areas, \n",
    "                   \"Perimeter\": perimeters, \n",
    "                   \"Circularity\": circularities,\n",
    "                   \"CenterX\": centers[:, 1], \n",
    "                   \"CenterY\": centers[:, 0]})\n",
    "df.index.name = \"Component\"\n",
    "# Sort values by area\n",
    "df = df.sort_values(by=\"Area\", ascending=False)\n",
    "\n",
    "display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation metrics**\n",
    "\n",
    "It is sometimes required to compare two masks. For instance, in the context of image segmentation,\n",
    "one might want to compare the ground truth mask with the predicted mask. Or one might want to compare\n",
    "two different structures in an image for their similarity. \n",
    "\n",
    "In the following, we are going to learn how to compare two masks using different metrics.\n",
    "\n",
    "Let's imagine that we have two masks $X$ and $Y$ that represent two shapes. Furthermore, let's assume that both masks consist of only one connected component, as illustrated below.\n",
    "\n",
    "We are going to work with the following popular metrics to compare two binary masks $X$ and $Y$:\n",
    "- [Dice coefficient](https://en.wikipedia.org/wiki/Dice-SÃ¸rensen_coefficient): \n",
    "     - in words: the size of the intersection divided by the average size of the two sets\n",
    "     - in formula: $D(X, Y) = \\frac{2|X \\cap Y|}{|X| + |Y|}$\n",
    "- [Intersection over Union (IoU)](https://en.wikipedia.org/wiki/Jaccard_index): \n",
    "     - in words: the size of the intersection divided by the size of the union\n",
    "     - in formula: $IoU(X, Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$ (also known as the Jaccard index)\n",
    "- [Hausdorff distance](https://en.wikipedia.org/wiki/Hausdorff_distance):\n",
    "     - in words: the largest minimal distance from a point in $X$ to a point in $Y$ or vice versa\n",
    "     - in formula: $H(X, Y) = \\max(\\max_{x \\in X} \\min_{y \\in Y} d(x, y), \\max_{y \\in Y} \\min_{x \\in X} d(x, y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a circle\n",
    "mask1 = np.zeros((300, 300), dtype=np.uint8)\n",
    "cv.circle(mask1, (150, 150), 120, 255, -1)\n",
    "\n",
    "# Draw a square.\n",
    "mask2 = np.zeros((300, 300), dtype=np.uint8)\n",
    "cv.rectangle(mask2, (50, 50), (250, 250), 255, -1)\n",
    "\n",
    "# Comparison\n",
    "mask_combine = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "mask_combine[..., 0] = mask1\n",
    "mask_combine[..., 1] = mask2\n",
    "\n",
    "tools.show_image_chain([mask1, mask2, mask_combine], \n",
    "                       titles=[\"Mask 1\", \"Mask 2\", \"Comparison\"], \n",
    "                       suppress_info=True, figsize=(6, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dice coefficient\n",
    "def dice(mask1, mask2):\n",
    "    \"\"\"Compute the Dice coefficient. The input masks should be binary.\"\"\"\n",
    "    assert mask1.shape == mask2.shape\n",
    "    assert mask1.dtype == bool and mask2.dtype == bool\n",
    "    intersection = mask1 & mask2  # Bitwise AND, equivalent to np.logical_and()\n",
    "    return 2*np.sum(intersection)/(np.sum(mask1) + np.sum(mask2))\n",
    "\n",
    "\n",
    "def iou(mask1, mask2):\n",
    "    assert mask1.shape == mask2.shape\n",
    "    assert mask1.dtype == bool and mask2.dtype == bool\n",
    "    intersection = mask1 & mask2  # Bitwise AND, equivalent to np.logical_and()\n",
    "    union = mask1 | mask2         # Bitwise OR, equivalent to np.logical_or()\n",
    "    return np.sum(intersection)/np.sum(union)\n",
    "\n",
    "\n",
    "def hausdorff(mask1, mask2):\n",
    "    assert mask1.shape == mask2.shape\n",
    "    from scipy.spatial import distance\n",
    "    ij1 = np.argwhere(mask1)  # Indices of the non-zero elements\n",
    "    ij2 = np.argwhere(mask2)  # Ditto\n",
    "    # Note: In general, the directed maximal distance from mask1 to mask2 is \n",
    "    #       not the same as the directed maximal distance from mask2 to mask1.\n",
    "    #       We take the maximum of the two.\n",
    "    ret12, _, _ = distance.directed_hausdorff(ij1, ij2)\n",
    "    ret21, _, _ = distance.directed_hausdorff(ij2, ij1)\n",
    "    return max(ret12, ret21)\n",
    "\n",
    "\n",
    "# Binarize the masks\n",
    "mask1 = mask1 > 0\n",
    "mask2 = mask2 > 0\n",
    "\n",
    "scores = {}\n",
    "scores[\"Dice\"] = dice(mask1, mask2)\n",
    "scores[\"IoU\"] = iou(mask1, mask2)\n",
    "scores[\"Hausdorff\"] = hausdorff(mask1, mask2)\n",
    "\n",
    "print(\"Scores for the two masks:\")\n",
    "print(\"=\"*25)\n",
    "print(pd.Series(scores).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results measure a good, but not perfect overlap.\n",
    "\n",
    "Let's now apply the comparison of masks to the red blood cells. To illustrate the process, we will first compute an alternative version of the segmentation mask we have used so far of the red blood cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold the image (Otsu's method)\n",
    "gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "_, mask_test = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "# Get rid of segmentation noise using morphological closing\n",
    "#mask_test = cv.morphologyEx(mask_test, cv.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "# Let's visualize the original image and the two masks\n",
    "tools.show_image_chain([img, mask, mask_test], \n",
    "                       titles=[\"Input\", \"Ground truth\", \"Otsu's method\"], \n",
    "                       suppress_info=True, figsize=(9, 3));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overall scores\n",
    "mask_bin = mask > 0\n",
    "mask_test_bin = mask_test > 0\n",
    "\n",
    "scores_overall = {}\n",
    "scores_overall[\"Dice\"] = dice(mask_bin, mask_test_bin)\n",
    "scores_overall[\"IoU\"] = iou(mask_bin, mask_test_bin)\n",
    "# --> This score is computationally expensive!\n",
    "scores_overall[\"Hausdorff\"] = hausdorff(mask_bin, mask_test_bin)\n",
    "print(\"Overall scores:\")\n",
    "print(\"=\"*25)\n",
    "print(pd.Series(scores_overall).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to identify those components that have a good overlap with the ground truth. We will identify the connected components in the two masks and then match them based on the Dice coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detail: cv.connectedComponents() requires an 8-bit image as input.\n",
    "components = cv.connectedComponents(mask, connectivity=8)[1]\n",
    "components_test = cv.connectedComponents(mask_test, connectivity=8)[1]\n",
    "\n",
    "# Problem: We need to match the components in the two masks, as the component\n",
    "#          IDs are arbitrary. We can do this by computing the Dice coefficient \n",
    "#          for each pair of components and then assign the component IDs based \n",
    "#          on the Dice coefficient.\n",
    "\n",
    "# Compute the Dice coefficient for each pair of components.\n",
    "dices = np.zeros((components.max()+1, components_test.max()+1))\n",
    "mask_match = np.zeros_like(img, dtype=np.uint8)\n",
    "for i in range(1, components.max()+1):\n",
    "    for j in range(1, components_test.max()+1):\n",
    "        mask_i = components == i\n",
    "        mask_j = components_test == j\n",
    "        dices[i, j] = dice(mask_i, mask_j)\n",
    "        \n",
    "# Method 1: Greedy matching: For each component of the ground truth, we assign\n",
    "#           the component of the test mask with the highest Dice coefficient.\n",
    "#           Note: It is possible \n",
    "mask_match = np.zeros_like(img, dtype=np.uint8)\n",
    "for i in range(1, components.max()+1):\n",
    "    j = np.argmax(dices[i, :])\n",
    "    if dices[i, j] > 0.95:\n",
    "        # Green: Very good match\n",
    "        mask_match[components_test == j] = [0, 255, 0]\n",
    "    elif dices[i, j] > 0.8:\n",
    "        # Yellow: Good match\n",
    "        mask_match[components_test == j] = [255, 255, 0]\n",
    "    elif dices[i, j] > 0.0:\n",
    "        # Red: Bad match\n",
    "        mask_match[components_test == j] = [255, 0, 0]\n",
    "        \n",
    "# # Method 2: Solve the assignment problem using the Hungarian algorithm.\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "row_ind, col_ind = linear_sum_assignment(-dices)\n",
    "mask_match = np.zeros_like(img, dtype=np.uint8)\n",
    "for i, j in zip(row_ind, col_ind):\n",
    "    if dices[i, j] > 0.95:\n",
    "        mask_match[components_test == j] = [0, 255, 0]\n",
    "    elif dices[i, j] > 0.8:\n",
    "        mask_match[components_test == j] = [255, 255, 0]\n",
    "    elif dices[i, j] > 0.0:\n",
    "        mask_match[components_test == j] = [255, 0, 0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dice coefficients:\")\n",
    "print(\"=\"*25)\n",
    "display(pd.DataFrame(dices).head(20).round(2))\n",
    "isp.show_image(mask_match, suppress_info=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TODO**: Mention Voronoi diagrams and Delaunay triangulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-isp-fs24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
