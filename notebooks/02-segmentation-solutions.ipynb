{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Segmentation â€“ Basics**\n",
    "\n",
    "In this notebook on segmentation, we will learn how to segment hematological images using different approaches. In a first step, we try to segment the cells using simple thresholding.\n",
    "\n",
    "Note that several of the topics discussed in this notebook are also covered \n",
    "in this insightful tutorial for this ImageJ/Fiji plugin [MorphoLibJ](https://imagej.net/plugins/morpholibj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Preparations**\n",
    "\n",
    "Let's begin with the usual preparatory steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "\n",
    "# Jupyter / IPython configuration:\n",
    "# Automatically reload modules when modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable vectorized output (for nicer plots)\n",
    "%config InlineBackend.figure_formats = [\"svg\"]\n",
    "\n",
    "#Â Inline backend configuration\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable this line if you want to use the interactive widgets\n",
    "# It requires the ipympl package to be installed.\n",
    "#%matplotlib widget\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, too, we will work with the same images as before in the notebook on pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "img1 = cv.imread(\"../data/images/hematology-baso1.jpg\", cv.IMREAD_COLOR)\n",
    "img2 = cv.imread(\"../data/images/hematology-baso2.jpg\", cv.IMREAD_COLOR)\n",
    "img3 = cv.imread(\"../data/images/hematology-blast1.jpg\", cv.IMREAD_COLOR)\n",
    "\n",
    "img1 = cv.cvtColor(img1, cv.COLOR_BGR2RGB)\n",
    "img2 = cv.cvtColor(img2, cv.COLOR_BGR2RGB)\n",
    "img3 = cv.cvtColor(img3, cv.COLOR_BGR2RGB)\n",
    "\n",
    "tools.show_image_chain([img1, img2, img3], titles=[\"img1\", \"img2\", \"img3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Thresholding**\n",
    "\n",
    "We can segment images using basic thresholding techniques. In this example, we explore the different thresholding methods available in OpenCV:\n",
    "- Simple thresholding (use [`cv.threshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57), use flag `cv.THRESH_BINARY` or `cv.THRESH_BINARY_INV`)\n",
    "- Adaptive thresholding (use [`cv.adaptiveThreshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#ga72b913f352e4a1b1b397736707afcde3))\n",
    "- Otsu's thresholding (use [`cv.threshold()`](https://docs.opencv.org/4.x/d7/d1b/group__imgproc__misc.html#gae8a4a146d1ca78c626a53577199e9c57), use flags `cv.THRESH_BINARY+cv.THRESH_OTSU`)\n",
    "\n",
    "Thresholding segments pixels into either foreground or background based on their intensity values. Thresholding is therefore an instance of *binary* image segmentation. The algorithms work by comparing the intensity values of the pixels in an image with a threshold value. Pixels with intensity values greater than the threshold are classified as foreground pixels, while pixels with intensity values less than the threshold are classified as background pixels. The threshold value can be set manually or determined automatically.\n",
    "\n",
    "As preparation, please read this documentation on segmentation methods in OpenCV: [Link](https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Choose here the image to work with\n",
    "img = img1\n",
    "\n",
    "# 1) Summarize the three different thresholding techniques in own words. Which methods\n",
    "#    use a global threshold, which ones apply a local threshold? \n",
    "\n",
    "# 2) Develop a strategy to segment the white blood cells (purple), the red blood cells \n",
    "# (red) and the background (white/gray). You may want to exploit the fact that we have\n",
    "# colors to work with:\n",
    "tools.show_image_chain([img[:,:,0], img[:,:,1], img[:,:,2]], titles=[\"R\", \"G\", \"B\"])\n",
    "\n",
    "# 3) Identify the different regions using thresholding\n",
    "mask_wbc = ...\n",
    "mask_rbc = ...\n",
    "mask_bg = ...\n",
    "\n",
    "# 4) Visualize the masks. Idea: combine the three masks into an RGB image.\n",
    "mask_seg = ...\n",
    "\n",
    "# 5) Discuss your results. What could be improved? What are the limitations of this \n",
    "#    approach? Are the masks mutually exclusive? Are they accurate?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# 1) cv.threshold:              Apply a global threshold to the image.\n",
    "#    cv.adaptiveThreshold:      Apply a local threshold to the image.\n",
    "#    cv.threshold (otsu):       Apply a global threshold to the image using Otsu's method.\n",
    "\n",
    "# 2) Segmentation strategy:\n",
    "#    The information in the three different channels suggests that we can use the\n",
    "#    red channel to segment the red blood cells, the blue channel to segment the white\n",
    "#    blood cells and the luminance channel (or gray channel) to segment the background. \n",
    "#    We can then combine the three masks to obtain the final segmentation.\n",
    "# \n",
    "#    Display the 3 channels. See note below as to why we disable normalization.\n",
    "img = img1\n",
    "tools.show_image_chain([img[:,:,0], img[:,:,1], img[:,:,2]], \n",
    "                       titles=[\"R\", \"G\", \"B\"], normalize=False)\n",
    "\n",
    "# 3) Let's try how this works in practice\n",
    "def segment_blood_cells_thr(img, return_masks=False):\n",
    "\n",
    "    # Smooth the image (to reduce noise)\n",
    "    #img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # -> Convert image to grayscale\n",
    "    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "    # -> Extract background using Otsu's method\n",
    "    thr_bg, mask_bg = cv.threshold(gray, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "    # -> Extract red and white blood cells\n",
    "    thr_rbc, mask_rbc = cv.threshold(img[:,:,0], 130, 255, cv.THRESH_BINARY)\n",
    "    thr_wbc, mask_wbc = cv.threshold(img[:,:,2], 140, 255, cv.THRESH_BINARY)\n",
    "\n",
    "    # -> Apply the segmentation logic: \n",
    "    #     - First observe that the background takes high values in the red and blue\n",
    "    #       channels. Thresholding the red and blue channels will also include the \n",
    "    #       background. Furthermore, the blue component sometimes is also present \n",
    "    #       in the red blood cells. Therefore:\n",
    "    #     - Exclude the background from the masks for the red and white blood cells\n",
    "    #     - Exclude the red blood cells from the white blood cells mask. Here,\n",
    "    #       we use the condition that something appears purple if the blue\n",
    "    #       channel is significantly higher than the red channel.\n",
    "\n",
    "    mask_rbc = mask_rbc.astype(bool) & ~mask_bg.astype(bool)\n",
    "    mask_wbc = mask_wbc.astype(bool) & ~mask_bg.astype(bool)\n",
    "    mask_wbc = mask_wbc & (img[:,:,0]*1.1 < img[:,:,2])\n",
    "\n",
    "    #Â Combine the information into a color image.\n",
    "    result = np.ones_like(img) * 255\n",
    "    result[mask_rbc.astype(bool)] = [155, 107, 132]\n",
    "    result[mask_wbc.astype(bool)] = [62, 32, 152]\n",
    "\n",
    "    if return_masks:\n",
    "        return result, mask_bg, mask_rbc, mask_wbc\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "# Compute the segmentation and viusalize the results\n",
    "img = img1\n",
    "ret = segment_blood_cells_thr(img, return_masks=True)\n",
    "result1, mask_bg, mask_rbc, mask_wbc = ret\n",
    "\n",
    "# Visualize the masks\n",
    "tools.show_image_chain([mask_bg, mask_rbc, mask_wbc], \n",
    "                       titles=[\"Background\", \"RBC\", \"WBC\"])\n",
    "# Visualize the results\n",
    "tools.show_image_chain([img, result1], titles=[\"Input: img1\", \"Output: Segmentation\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation looks fairly good, but it is not perfect. The main limitations are:\n",
    "- The thresholding is very sensitive to the threshold values. Small changes in the threshold values can lead to very different results.\n",
    "- The assumptions and the segmentation logic are very specific to the images at hand. They may not generalize well to other images.\n",
    "- The masks contain holes and do not perfectly segment the cells. This is due to the thresholding method used.\n",
    "- There are boundary effects visible (e.g. in the red blood cells), indicating that the thresholding strategy is not perfect.\n",
    "- Two nearby cells may touch or overlap. In that case we cannot distinguish them in theÂ segmentation. The result for `img3` shows this problem clearly, see below.\n",
    "\n",
    "We can refine the results of this approach by...\n",
    "- ...tuning the thresholding parameters\n",
    "- ...by improving the segmentation logic\n",
    "- ...by smoothing the image (see commented out line of code above)\n",
    "- ...by using morphological operations (to close holes and remove noise in the segmentation masks, see next tutorial).\n",
    "\n",
    "**Note**: When displaying the images, it is helpful to disable normalization, which changes the appearance of the image by stretching the pixel values to the full range of [0, 255]. For (single-channel) images, this is the default behavior of [`plt.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html), the underlying function we use in our convenience function `tools.show_image_chain()`. We want to keep the original values so that we can read the grayscale values from the image with a color picker. \n",
    "\n",
    "A color picker is a tool for measuring the current color value under the mouse pointer. On MacOS there is the [Digital Color Meter](https://support.apple.com/guide/digital-color-meter/welcome/mac) (installed by default under /System/Applications/Utilities/). On Windows, there is an equivalent tool *Color Picker* as part of the [PowerToys](https://learn.microsoft.com/en-us/windows/powertoys/). Ubuntu offers [Gpick](https://www.gpick.org/). These tools can display the color values in different formats, and you can use them to copy color values to the clipboard, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the segmentation results also for the other images:\n",
    "result2 = segment_blood_cells_thr(img2)\n",
    "tools.show_image_chain([img2, result2], titles=[\"Input: img2\", \"Output: Segmentation\"]);\n",
    "result3 = segment_blood_cells_thr(img3)\n",
    "tools.show_image_chain([img3, result3], titles=[\"Input: img3\", \"Output: Segmentation\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## **Color clustering**\n",
    "\n",
    "Instead of segmenting the image, we could also try to classify the different regions. For this, we can use a clustering algorithm such as [K-means](https://en.wikipedia.org/wiki/K-means_clustering). The algorithm works by classifying pixels in an image into clusters based on their color similarity. The similarity is measured using the (Eucledian or non-Euclidean) distance between the pixel values. The algorithm iteratively assigns pixels to clusters based on their distance to the cluster centers and updates the cluster centers based on the mean of the pixels in the cluster. The process continues until the cluster centers converge. See here for a nice [visualization](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)) of the algorithm.\n",
    "\n",
    "As a preparatory step, have a look at the following two tutorials:\n",
    "- Machine Learning Master / Jason Brownlee on [color quantization](https://machinelearningmastery.com/k-means-clustering-in-opencv-and-application-for-color-quantization/)\n",
    "- Shubhang Agrawal / Image segmentation using [k-means clustering](https://medium.com/swlh/image-segmentation-using-k-means-clustering-46a60488ae71) (the tutorial has a few flaws, please excuse). \n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "Resources:\n",
    "# Nice way of depicting the bars\n",
    "https://pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/\n",
    "# OpenCV\n",
    "https://docs.opencv.org/3.4/d1/d5c/tutorial_py_kmeans_opencv.html\n",
    "# Machine Learning Mastery\n",
    "https://machinelearningmastery.com/k-means-clustering-in-opencv-and-application-for-color-quantization/\n",
    "# Watershed\n",
    "https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "# Segmentation with Skimage \n",
    "https://github.com/ipython-books/cookbook-2nd-code/blob/master/chapter11_image/03_segmentation.ipynb\n",
    "# Combination between thresholding and color clustering\n",
    "https://towardsdatascience.com/image-color-segmentation-by-k-means-clustering-algorithm-5792e563f26e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "# Choose here the image to work with\n",
    "img = img1\n",
    "\n",
    "# 1) Reshape the color pixels into a Mx3 matrix (M: number of pixels)\n",
    "#    and convert the data type to float32.\n",
    "data = img.reshape(-1, 3).astype(np.float32)\n",
    "\n",
    "# 2) Apply the K-means algorithm to the data. Use the cv.kmeans function.\n",
    "#    Choose the number of clusters K=3.\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "K = 3\n",
    "ret, label, centers = cv.kmeans(data, K, None, criteria, 10, cv.KMEANS_RANDOM_CENTERS)\n",
    "# label contains the cluster index for each pixel\n",
    "# centers contains the cluster centers (colors!)\n",
    "\n",
    "# 3) Reshape and convert the data back to uint8\n",
    "img_seg = ...\n",
    "\n",
    "# 4) Visualize the segmented image\n",
    "tools.show_image_pair(img, img_seg, title1=\"Original\", title2=\"Segmented\");\n",
    "\n",
    "# 5) Repeat the process for a different color space (e.g. HSV)\n",
    "#    Is the clustering more robust? Why? When does this approach fail?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "def segment_blood_cells_kmeans(img, K=3, use_lab=False):\n",
    "    # Blur the image to reduce noise (step is required here \n",
    "    # to yield feasible results)\n",
    "    img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    if use_lab:\n",
    "        img = cv.cvtColor(img, cv.COLOR_RGB2LAB)\n",
    "\n",
    "    # 1) Reshape the color pixels into a Mx3 matrix (M: number of pixels)\n",
    "    #    and convert the data type to float32.\n",
    "    data = img.reshape(-1, 3).astype(np.float32)\n",
    "    \n",
    "    # 2) Apply the K-means algorithm to the data. Use the cv.kmeans function.\n",
    "    # Some parameters for the kmeans algorithm (termination criteria):\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
    "    ret, label, centers = cv.kmeans(data, \n",
    "                                    K=K, \n",
    "                                    bestLabels=None, \n",
    "                                    criteria=criteria, \n",
    "                                    attempts=10, \n",
    "                                    flags=cv.KMEANS_PP_CENTERS)\n",
    "    # label contains the cluster index for each pixel\n",
    "    # centers contains the cluster centers (colors!)\n",
    "\n",
    "    # 3) Reshape and convert the data back to uint8\n",
    "    img_seg = centers[label.flatten()].reshape(img.shape).astype(np.uint8)\n",
    "\n",
    "    if use_lab:\n",
    "        img_seg = cv.cvtColor(img_seg, cv.COLOR_LAB2RGB)\n",
    "\n",
    "    # 4) Return the segmented image\n",
    "    return img_seg\n",
    "\n",
    "\n",
    "img = img1\n",
    "img_seg = segment_blood_cells_kmeans(img, K=3, use_lab=False)\n",
    "tools.show_image_chain([img, img_seg], titles=[\"Original\", \"Segmented\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here looks somewhat better than in the previous example, however,\n",
    "the segmentation is still not perfect:\n",
    "- It dependends on the prevalence of the colors in the image how well the clustering will work. If there are more nuances in the (e.g., background\n",
    "  color) the clustering will yield more clusters representing the background, instead of identifying the blood cells.\n",
    "- The K-means algorithm is sensitive to the initialization of the cluster centers. If the initialization is not done properly, the algorithm might   get stuck in a local minimum. \n",
    "- We still suffer in presence of touching and overlapping cells. While we can assign the right color to the cells, we cannot separate them.\n",
    "\n",
    "We may be able to improve the results by using a different color space. For example, the LAB color space is more robust to changes in illumination and is more closely related to the human perception of color. Furthermore, we can allow more clusters to be found by the algorithm and then apply some post-processing to merge clusters that are similar. For example, if the cluster centers (= colors) have a dominant blue component, we can merge them into one cluster), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Watershed algorithm for segmentation**\n",
    "\n",
    "The watershed algorithm is a powerful tool for image segmentation. It is based on\n",
    "the concept of watershed lines, which are lines that separate different regions in\n",
    "an image. The algorithm works by flooding the image with \"water\" from different\n",
    "regions (or seed points) and letting the water flow until it reaches a boundary. \n",
    "The watershed lines are then defined as the boundaries between the regions where\n",
    "the water meets. The algorithm can be used to segment images into different regions\n",
    "based on the intensity or color of the pixels. It is particularly useful for\n",
    "segmenting images with complex shapes and structures.\n",
    "\n",
    "Our dataset structurally resembles the image used in this [tutorial](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html). \n",
    "Let's use this demo naively to segment our image using the watershed method. \n",
    "\n",
    "The tutorial uses the following strategy: \n",
    "1. Convert the image into binary mask using thresholding\n",
    "2. Apply so-called morphological operations\\* to remove noise, and to separate the objects. We also use similar operations to identify regions that most likely represent the background of our scene.\n",
    "3. with the next steps we aim to identify the seed points for the watershed algorithm\n",
    "   1. Apply the distance transform\\*\\* to the binary image, which measures the distance of each pixel to the nearest zero pixel.\n",
    "   2. Apply a threshold to the distance-transformed image to obtain blobs of pixels that are located close to the centers of our objects of interest.\n",
    "   3. Use the connected components\\*\\*\\* algorithm to identify and enumerate the different seed points. This turns \n",
    "  the binary mask into a mask with (integer) labeled objects.\n",
    "   4. In a last step, mark the background seed regions (see step 2.) with label 0.\n",
    "4. Fiinally, apply the watershed algorithm to segment the regions of interest.\n",
    "5. Visualize the segmented image.\n",
    "\n",
    "\n",
    "### \\* **Morphological operations** \n",
    "Morphological operations are a set of operations that are used to analyze and manipulate\n",
    "the shape of objects in an image. Although the operations are defined also for other types\n",
    "of images, they are most commonly used in binary images. The operations involve structuring\n",
    "elements (kernels) that are used to probe the image and modify the pixel values based on the\n",
    "interaction between the kernel and the image. The most common morphological operations are\n",
    "dilation (expand the shapes), erosion (shrink the shapes), opening (dilation followed by\n",
    "erosion), and closing (erosion followed by dilation). Morphological operations are used to \n",
    "remove noise, separate objects, and connect objects in an image. \n",
    "\n",
    "**Further reading**:\n",
    "- OpenCV documentation. [Link](https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html)\n",
    "- Beautiful illustration how morphological operations work: [Link](https://penny-xu.github.io/blog/mathematical-morphology)\n",
    "- Wikipedia article on mathematical morphology. [Link](https://en.wikipedia.org/wiki/Mathematical_morphology)\n",
    "- Blog post on morphological operations. [Link](https://towardsdatascience.com/7bcf1ed11756)\n",
    "\n",
    "### \\*\\* **Distance transform**\n",
    "The distance transform is useful for a variety of applications in image processing. It\n",
    "permits to compute the distance of each pixel to the nearest boundary in a binary image.\n",
    "Based on distance transforms, we can perform operations such as skeletonization, identify\n",
    "geometric properties of objects or segment images. The algorithm works by iteratively \n",
    "propagating the distance values from the boundary pixels to the interior pixels. The \n",
    "distance is computed using a metric such as the Euclidean distance. It is relatively \n",
    "cheap to compute the distance transform.\n",
    "\n",
    "**Further reading**\n",
    "- Another application of the distance transform (in combination with watershed). [Link](https://docs.opencv.org/3.4/d2/dbd/tutorial_distance_transform.html)\n",
    "\n",
    "\n",
    "### \\*\\*\\* **Connected components**\n",
    "Connected components in a binary image are regions of pixels that are touching each\n",
    "other. In image processing, connected components are used to identify single objects \n",
    "in a segmentation mask. The connected components algorithm works by labeling each\n",
    "pixel in the mask with a unique (integer) label based on the connectivity of the pixels. \n",
    "\n",
    "**Further reading**\n",
    "- Wikipedia article on connected component labeling. [Link](https://en.wikipedia.org/wiki/Connected-component_labeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "\n",
    "img = img1\n",
    "\n",
    "# Implement the approach lined out above. You can copy paste the code \n",
    "# from the above link and plug our image into it. Try to understand the\n",
    "# code and the different steps. You may have to adjust the parameters\n",
    "# to get a good segmentation result.\n",
    "\n",
    "# https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "def segment_red_blood_cells_watershed(img):\n",
    "    \n",
    "    img = cv.GaussianBlur(img, (5, 5), 0)\n",
    "    img_blur = cv.medianBlur(img, 5)\n",
    "    \n",
    "    #tools.show_image(img_blur)\n",
    "    \n",
    "    gray = cv.cvtColor(img_blur, cv.COLOR_RGB2GRAY)\n",
    "    gray = img_blur[:,:,0]\n",
    "    ret, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "\n",
    "    # Noise removal\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations=9)\n",
    "\n",
    "    # Sure background area\n",
    "    sure_bg = cv.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "    thr = 0.1 * dist_transform.max()\n",
    "    thr = 18\n",
    "    ret, sure_fg = cv.threshold(dist_transform, thr, 255, 0)\n",
    "    \n",
    "    tools.show_image_chain([sure_fg, sure_bg], titles=[\"Sure FG\", \"Sure BG\"])\n",
    "    tools.show_image_chain([opening, dist_transform], titles=[\"Opening\", \"Distance transform\"])\n",
    "\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg, sure_fg)\n",
    "    \n",
    "    # Marker labelling\n",
    "    ret, markers = cv.connectedComponents(sure_fg)\n",
    "    \n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers+1\n",
    "    \n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown==255] = 0\n",
    "    \n",
    "    markers = cv.watershed(img,markers)\n",
    "    img[markers == -1] = [255,0,0]\n",
    "    \n",
    "    return markers, img\n",
    "    \n",
    "    \n",
    "img = img1.copy()\n",
    "markers, result = segment_red_blood_cells_watershed(img=img)\n",
    "tools.show_image_chain([markers, result], \n",
    "                       titles=[\"Markers\", \"Segmented\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **AI driven segmentation**\n",
    "Nowadays, deep learning is often used for image segmentation. The U-Net\n",
    "architecture is a popular choice for this task. It is a convolutional\n",
    "neural network that is particularly well suited for segmentation tasks.\n",
    "The nnU-Net is a more advanced version of the U-Net that has been optimized\n",
    "for medical image segmentation, and comes with self-adapting preprocessing\n",
    "and postprocessing steps, where the network learns the optimal parameters\n",
    "for the task at hand. It is available as a Python package and can be installed\n",
    "via pip.\n",
    "\n",
    "Although machine learning / artificial intelligence is not the focus of this\n",
    "course, we can use such models to perform image segmentation. In contrast to \n",
    "the methods described above, deep learning models can learn the relevant \n",
    "features from the data and can generalize to unseen data. However, these methods\n",
    "require large amounts of labeled data for training, and are computationally\n",
    "more expensive. They are also often considered as \"black boxes\", as it is\n",
    "difficult to understand why the model makes a certain decision.\n",
    "\n",
    "```python\n",
    "######################\n",
    "###    EXCERISE    ###\n",
    "######################\n",
    "```\n",
    "\n",
    "Visit the following resources and examine if they could be useful for your own segmentation project.\n",
    "- Segment anything by Meta AI. [Demo](https://segment-anything.com/demo), [Paper](https://arxiv.org/abs/2304.02643), [Code](https://github.com/facebookresearch/segment-anything) \n",
    "- Huggingface: Collection of public, pre-trained models. [Link](https://huggingface.co/spaces).\n",
    "  - Some models come with a demo interface. \n",
    "  - For instance, the background removal [RemBG](https://huggingface.co/spaces/KenjieDec/RemBG) tool is found there.\n",
    "  - Another popular segmentation tool is [Demo](https://huggingface.co/spaces/fcakyon/yolov8-segmentation), [Code](https://huggingface.co/spaces/fcakyon/yolov8-segmentation)\n",
    "  - To search the entire database for models: [Link](https://huggingface.co/models)\n",
    "- Total segmentator for anatomical CT data. [Demo](https://totalsegmentator.com/), [Paper](https://arxiv.org/abs/2208.05868), [Code](https://github.com/wasserth/TotalSegmentator)\n",
    "\n",
    "\n",
    "We have seen now a couple of approaches to segment images. How you can make best\n",
    "use of them depends on the specific problem and your engineering skills. ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "###    SOLUTION    ###\n",
    "######################\n",
    "\n",
    "# Let's use the Segment Anything model from Meta.\n",
    "# The following lines may take a while to execute.\n",
    "try:\n",
    "    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "except ImportError:\n",
    "    print(\"Installing the model...\")\n",
    "    !pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
    "    !pip install -q opencv-python pycocotools matplotlib onnxruntime onnx\n",
    "    !pip install -q torch torchvision\n",
    "\n",
    "# Download the model (if not available yet)\n",
    "path_to_checkpoint=\"./sam_vit_h_4b8939.pth\"\n",
    "url_checkpoint=\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "if not Path(path_to_checkpoint).exists():\n",
    "    print(\"Downloading model... This may take a while!\")\n",
    "    !wget -O {path_to_checkpoint} {url_checkpoint}\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=path_to_checkpoint)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(img, overlay):\n",
    "    \"\"\"Blend an image with an overlay.\"\"\"\n",
    "    alpha = overlay[:,:,3]\n",
    "    img = img.astype(np.float32)\n",
    "    result = ((1 - alpha[:, :, None]) * img + \n",
    "              alpha[:, :, None] * overlay[:, :, :3] * 255)\n",
    "    result = result / result.max()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_blood_cells_sam(img, masks):\n",
    "    result = img.copy()\n",
    "    overlay_color = [255, 255, 0]\n",
    "    alpha = 0.2\n",
    "    clean_masks = True\n",
    "\n",
    "    # Sort masks by area\n",
    "    masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "\n",
    "    if clean_masks:\n",
    "        # Filter masks that are fully mask\n",
    "        to_remove = []\n",
    "        for i, m1 in enumerate(masks):\n",
    "            m1 = m1[\"segmentation\"]\n",
    "            for j in range(i+1, len(masks)):\n",
    "                m2 = masks[j][\"segmentation\"]\n",
    "                if i in to_remove or j in to_remove:\n",
    "                    continue\n",
    "                if (m1.sum()) ==( (m1 | m2).sum()):\n",
    "                    to_remove.append(j)\n",
    "        masks = [m for i, m in enumerate(masks) if i not in to_remove]\n",
    "        \n",
    "    # Check the type of cell, using the following heuristic:\n",
    "    # If the mask is mostly red, it is a red blood cell, if\n",
    "    # it is mostly blue, it is a white blood cell\n",
    "    for m in masks:\n",
    "        mask = m[\"segmentation\"]\n",
    "        r = img[:,:,0][mask].mean()\n",
    "        b = img[:,:,2][mask].mean()\n",
    "        m[\"type\"] = \"rbc\" if r > b else \"wbc\"\n",
    "        \n",
    "    # Visualize the masks\n",
    "    result = np.ones((img.shape[0], img.shape[1], 4))\n",
    "    result[:,:,3] = 0\n",
    "    for m in masks:\n",
    "        cell = m[\"type\"]\n",
    "        m = m[\"segmentation\"]\n",
    "        contours, hierarchy = cv.findContours(m.astype(np.uint8)*255, \n",
    "                                                cv.RETR_TREE, \n",
    "                                                cv.CHAIN_APPROX_SIMPLE)\n",
    "        # Random numbers for the color\n",
    "        rr, rb, rg = np.random.random(3)*0.5\n",
    "        overlay_color = ([255/255, rb, rg, alpha] if (cell == \"rbc\") \n",
    "                         else [rr, rg, 255/255, alpha])\n",
    "        result[m] = overlay_color\n",
    "        overlay_color[3] = 1\n",
    "        cv.drawContours(result, contours, -1, overlay_color, 2)\n",
    "        \n",
    "    result = blend(img, result)\n",
    "    tools.show_image_chain([img, result], titles=[\"Input\", \"Output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image here:\n",
    "img = img1\n",
    "\n",
    "#Â Generate the masks (this may take a while, about 30s)\n",
    "# Keeping this call outside the segment* function because \n",
    "# this step takes a while.\n",
    "print(\"Generating mask... This may take a while!\")\n",
    "masks = mask_generator.generate(img)\n",
    "\n",
    "# Visualize the masks\n",
    "segment_blood_cells_sam(img, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary mask for the red blood cells and save it to a file\n",
    "mask = sum([m[\"segmentation\"] for m in masks if m.get(\"type\") == \"rbc\"])\n",
    "# Apply some morphological opening to clean the mask\n",
    "mask = cv.morphologyEx(mask.astype(np.uint8), cv.MORPH_OPEN, \n",
    "                       np.ones((3, 3), np.uint8), iterations=1)\n",
    "tools.show_image(mask.astype(np.uint8)*255, title=None, suppress_info=True)\n",
    "cv.imwrite(\"mask_rbc.png\", mask.astype(np.uint8)*255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks very good now. The model is able to segment the \n",
    "different cells very well. This is quite impressive, considering that\n",
    "the model was trained on a general purpose dataset and not on medical\n",
    "images. The model is able to generalize well to new domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for img2\n",
    "img = img2\n",
    "masks = mask_generator.generate(img)\n",
    "segment_blood_cells_sam(img, masks)\n",
    "\n",
    "# Repeat for img3\n",
    "img = img3\n",
    "masks = mask_generator.generate(img)\n",
    "segment_blood_cells_sam(img, masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
